\documentclass[
  man,
  floatsintext,
  longtable,
  nolmodern,
  notxfonts,
  notimes,
  colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{apa7}

\usepackage{amsmath}
\usepackage{amssymb}




\RequirePackage{longtable}
\RequirePackage{threeparttablex}

\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
	{0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
	{-.5em}%
	{\normalfont\normalsize\bfseries\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{0.5em}%
	{0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
	{-\z@\relax}%
	{\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother




\usepackage{longtable, booktabs, multirow, multicol, colortbl, hhline, caption, array, float, xpatch}
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.7}

\usepackage{tcolorbox}
\tcbuselibrary{listings,theorems, breakable, skins}
\usepackage{fontawesome5}

\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{ACACAC}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582EC}
\definecolor{quarto-callout-important-color-frame}{HTML}{D9534F}
\definecolor{quarto-callout-warning-color-frame}{HTML}{F0AD4E}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02B875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{FD7E14}

%\newlength\Oldarrayrulewidth
%\newlength\Oldtabcolsep


\usepackage{hyperref}



\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}

\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother


% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}


\usepackage[nolongtablepatch]{lineno}
\linenumbers



\usepackage{newtx}

\defaultfontfeatures{Scale=MatchLowercase}
\defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}





\title{eyetools: an R package for open-source analysis of eye data}


\shorttitle{eyetools: eye data analysis}


\usepackage{etoolbox}








\authorsnames{Tom Beesley,Matthew Ivory}





\affiliation{
{Lancaster University}}




\leftheader{Beesley and Ivory}



\abstract{Abstract goes here}

\keywords{eye-tracking; fixations; saccades; areas-of-interest}

\authornote{\par{\addORCIDlink{Tom Beesley}{0000-0003-2836-2743}} 

\par{       }
\par{Correspondence concerning this article should be addressed to Tom
Beesley, Lancaster University, Department of Psychology, Lancaster
University, UK, LA1 4YD, UK, Email: t.beesley@lancaster.ac.uk}
}

\makeatletter
\let\endoldlt\endlongtable
\def\endlongtable{
\hline
\endoldlt
}
\makeatother

\urlstyle{same}



\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\makeatother

% From https://tex.stackexchange.com/a/645996/211326
%%% apa7 doesn't want to add appendix section titles in the toc
%%% let's make it do it
\makeatletter
\xpatchcmd{\appendix}
  {\par}
  {\addcontentsline{toc}{section}{\@currentlabelname}\par}
  {}{}
\makeatother

%% Disable longtable counter
%% https://tex.stackexchange.com/a/248395/211326

\usepackage{etoolbox}

\makeatletter
\patchcmd{\LT@caption}
  {\bgroup}
  {\bgroup\global\LTpatch@captiontrue}
  {}{}
\patchcmd{\longtable}
  {\par}
  {\par\global\LTpatch@captionfalse}
  {}{}
\apptocmd{\endlongtable}
  {\ifLTpatch@caption\else\addtocounter{table}{-1}\fi}
  {}{}
\newif\ifLTpatch@caption
\makeatother

\begin{document}

\maketitle


\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

\setlength\LTleft{0pt}

\resetlinenumber[1]

Eye tracking is now an established and widely used technique in the
behavioural sciences. Psychology is perhaps the scientific discipline
which has seen the most sustantial adoption of eye-data research, where
eye-tracking systems are now commonplace in centres of academic
research. Beyond academic institutions, eye-tracking continues to be a
useful tool in understanding consumer behaviour, user-interface design,
and in various forms of entertainment.

By recording the movement of an individual's gaze during research
studies, researchers can quantify where and how long individual's look
at particular regions of space (usually with a focus on stimuli
presented on a 2D screen, but also within 3D space). Eye-tracking
provides a rich stream of continuous data and therefore can offer
powerful insights into real-time cognitive processing. Such data allow
researchers to inspect the interplay of cognitive processes such as
attention, memory, and decision making, with high temporal precision
(\citeproc{ref-beesley2019}{Beesley et al., 2019}).

While there are abundant uses and benefits of collecting eye-movement
data in psychology experiments, the continual stream of recording can
lead to an overwhelming amount of raw data: modern eye-trackers can
record data at 1000 Hz and above, which results in 3.6 million rows of
data per hour. The provision of suitable computational software for data
reduction and processing is an important part of eye-tracking research.
The companies behind eye-tracking devices offer licensed software that
will perform many of the necessary steps for eye-data analysis. However,
there are several disadvantages to using such proprietary software in a
research context. Firstly, the software will typically have an ongoing
license cost for continual use. Secondly, the algorithms driving the
operations within such software are not readily available for
inspection. Both of these important constraints mean that the use of
proprietary analysis software will lead to a failure to meet the basic
open-science principle of analysis reproduction, for example as set out
by the UK Reproducibility Network: ``We expect researchers to\ldots{}
make their research methods, software, outputs and data open, and
available at the earliest possible point\ldots The reproducibility of
both research methods and research results \ldots is critical to
research in certain contexts, particularly in the experimental sciences
with a quantitative focus\ldots{}''

In the current article we introduce a new toolkit for eye-data
processing and analysis called ``\emph{eyetools}'', which takes the form
of an R package. R packages (like R itself) are free to use without
licence and are therefore available for any user across the world. The
package provides a (growing) number of functions that provide an
efficient and effective means to conduct basic eye-data analysis.
\emph{eyetools} is built with academic researchers in the psychological
sciences in mind, though there is no reason why the package would not be
effective more generally. The functions within the package reflect steps
in a comprehensive analysis workflow, taking the user from initial
handling of raw eye data, to summarising data for each period of a
procedure, to the visualisation of the data in plots. We hope that the
functions are simple enough to mean that the package is easy to use for
researchers who are unfamiliar with working with eye data. It should
also appeal to researchers accustomed to working with eye data in other
environments who wish to transfer to working in R.

\emph{eyetools} is, of course, not the only package in R that allows
users to work with eye data. A recent assessment of available packages
on CRAN identified six other packages that offer relevant functions for
the analysis of eye data. \textbf{eyeTrackr}, \textbf{eyelinker}, and
\textbf{eyelinkReader}, all offer functionality for data only from
experiments that have used `EyeLink' trackers (S-R Research). In
contrast, eyetools provides functions that are hardware-agnostic,
relying on a format of data that can be achieved from any data source.
The \textbf{eyeRead} package is designed for the analysis of eye data
from reading exercises. The \textbf{emov} package offers a limited set
of functions and is primarily designed for fixation detection, using the
same dispersion method employed in eyetools. Finally,
\textbf{eyetrackingR} is perhaps the most comprehensive alternative
package available on CRAN. eyetrackingR offers a large suite of
functionality and, like eyetools, can be applied across the entire
pipeline. It has functions for cleaning data and various plotting
functions, including analysis over time. It does not feature algorithms
regarding the detection of events such as saccades or fixations. This
limits the ability to conduct more bespoke analysis steps and it means
that analysis needs to be conducted on raw data. This is disadvantageous
both in terms of computing time and in the open sharing of data (event
data are an order of magnitude smaller in size than raw data). In
comparison, eyetools enables easier data processing up to data analyses,
making core tasks in the eye data processing easier and standardised.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1429}}
  >{\centering\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1429}}
  >{\centering\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1429}}
  >{\centering\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1429}}
  >{\centering\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1429}}
  >{\centering\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1429}}
  >{\centering\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1429}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Package
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Hardware-agnostic
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Data Import
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Data processing
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Identifies events
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Plotting
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Inferential Analysis
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
eyetools & \faIcon{check} & \faIcon{check}* & \faIcon{check} &
\faIcon{check} & \faIcon{check} & \\
eyeTrackr & & \faIcon{check} & \faIcon{check} & \faIcon{check} & & \\
eyelinker & & \faIcon{check} & & & & \\
eyelinkReader & & \faIcon{check} & & \faIcon{check} & \faIcon{check}
& \\
eyeRead & \faIcon{check} & & \faIcon{check} & \faIcon{check}** & & \\
emov & \faIcon{check} & & \faIcon{check} & \faIcon{check} & & \\
eyetrackingR & \faIcon{check} & & \faIcon{check} & & \faIcon{check} &
\faIcon{check} \\
\end{longtable}

* for Tobii data only, ** for text reading experiments only

In this tutorial we demonstrate the pipeline of analysis functions
within \emph{eyetools}. The package has been designed to be simple to
use by someone with basic knowledge of data handling and analysis in R.
This tutorial is separated into five distinct sections. In the first
section, we briefly describe the basic methodology of collecting eye
data in general, and in regard to the specific dataset we use to
illustrate all the functionality of the \emph{eyetools} package. The
second section covers the process for getting data from an eye tracker
into an \emph{eyetools}-friendly format. The third section introduces
the foundational functions of the \emph{eyetools} package, from
repairing and smoothing eye data, to calculating fixations and saccades,
and detecting time spent in Areas of Interest (AOIs). The fourth section
takes the processed data, and applies basic analysis techniques
commonplace in eye data research. In the fifth and final section, we
reflect on the benefits of the \emph{eyetools} package, including
contributions to open science practices, reproducibility, and providing
clarity to eye data analysis.

\subsection{Installing eyetools}\label{installing-eyetools}

\emph{eyetools} is available on CRAN and can be installed with the
command \texttt{install.packages("eyetools")}. Instructions for
installing development versions can be found at the package repository:
\url{https://github.com/tombeesley/eyetools/}. Once installed, the
package can be loaded into R with the command
\texttt{library(eyetools)}.

\subsection{Preparing data for
eyetools}\label{preparing-data-for-eyetools}

Since there is a wide range of eye tracking hardware available for
researchers to use, \emph{eyetools} currently offers only a limited
number of functions for converting raw data from specific hardware. The
\texttt{hdf5\_to\_dataframe()} function is designed to work with output
from PsychoPy experiments connected to modern Tobii hardware, and will
take this default format of raw data and convert it into the simplified
raw data format required for \emph{eyetools}.

The \emph{eyetools} package has been developed primarily with the
analysis of experimental psychology data in mind. To this end, many of
the functions expect a ``trial'' variable in the data, such that the
algorithms will operate over multiple trials and produce output that
retains this trial information. Similarly, data in psychology
experiments tends to come from multiple participants, and to facilitate
analysis, a participant ID column can be included (though this isn't
necessary). This allows many functions to be run automatically across
multiple participants (rather than running the same function on each
participant's data). It is also necessary to select the relevant
``periods'' of data within the recording. It is quite typical in
psychology experiments for there to be multiple periods within a trial,
e.g., fixation; stimulus presentation; response feedback;
inter-trial-interval. \emph{eyetools} does not interpret these changes,
and so it is necessary to first select the data for the period or
periods that are of interest for analysis. Analysis on each period would
be conducted separately using the functions in \emph{eyetools}.

The starting point for the analysis pipeline is the preparation of the
raw eye data, which will consist of recorded samples from the
eye-tracker, with each row in the data reflecting a single time-stamped
recording. If the eye-tracker is set at 1000Hz, then consecutive
recordings will be 1 millisecond of time apart; at 300Hz, the recordings
are 3.33 milliseconds apart. The only requirement for the time column is
that the values reflect a consistent and increasing set of values. There
is no need to specify the sampling rate, since \emph{eyetools} functions
will calculate this automatically. \emph{eyetools} expects raw data to
have the following columns:

\begin{itemize}
\item
  x = horizontal spatial coordinate of the estimated eye position
\item
  y = vertical spatial coordinate of the estimated eye position
\item
  time = timestamp of the recording
\item
  trial = the index of the current trial in the data
\item
  pID = the unique identifier for the data from each participant
\end{itemize}

The first four columns should be set as type numeric, while ``pID'' can
be numeric, character, or factor. The order of the columns is not
important. Missing values in the x and y columns of the raw data must be
expressed as ``NA''.

For many of the functions in \emph{eyetools} the \emph{pID} column
allows the user to run the function across data from multiple
participants. This functionality means that the user can avoid having to
generate additional programming steps to analyse and combine the data
from multiple participants.

For many methods of eye-tracking, binocular data will be produced. In
such cases, since the primary aim of our analyses is the estimation of
the spatial coordinates of gaze, the function \texttt{combine\_eyes()}
should be used to combine the data to form a set of monocular data. This
function takes raw data with coordinates for each eye (i.e., left\_x,
right\_x, left\_y, right\_y), and converts the data into single x and y
coordinates. By default, the function does this by taking an average of
the coordinates from the two eyes of each timestamp, but it is also
possible to select data from the eye with the lowest proportion of
missing samples. This returns a flattened list of participant data that
has x and y variables in place of the left\_* and right\_* variables.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(HCL,}\DecValTok{4}\NormalTok{) }\CommentTok{\# first 4 rows of the built{-}in data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 7
  pID    time left_x left_y right_x right_y trial
  <chr> <dbl>  <dbl>  <dbl>   <dbl>   <dbl> <dbl>
1 118       0   909.   826.   1003.    808.     1
2 118       3   912.   829.   1001.    812.     1
3 118       7   912.   826.   1010.    813.     1
4 118      10   908.   824.   1006.    807.     1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{combine\_eyes}\NormalTok{(HCL) }\CommentTok{\# create monocular data }

\FunctionTok{head}\NormalTok{(data, }\DecValTok{4}\NormalTok{) }\CommentTok{\# first 4 rows of the monocular data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  pID time trial        x        y
1 118    0     1 955.8583 816.5646
2 118    3     1 956.5178 820.6221
3 118    7     1 960.7383 819.7616
4 118   10     1 956.9727 815.3331
\end{verbatim}

\subsection{Repairing missing data and smoothing
data}\label{repairing-missing-data-and-smoothing-data}

Despite the best efforts of the researcher, there are occasional
failures in the accurate recording of the eye position during data
collection (e.g., blinks). This results in missing data within the
stream of samples, which must be represented in eyetools as NA values
for the x and y coordinates. To mitigate the impact of missing data on
further analysis, the \texttt{interpolate()} function can estimate the
missing gaze data, based upon the eye coordinates before and after the
missing data, and perform a repair. The default method of linear
interpolation (``approx'') replaces missing values with a line of
constant slope and evenly spaced coordinates reaching the existing data
(alternatively a cubic ``spline'' method can be applied).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{interpolate}\NormalTok{(data, }
                    \AttributeTok{method =} \StringTok{"approx"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

When using \texttt{interpolate()}, a report can be requested on the
proportion of missing data that has been replaced. This parameter
changes the output format of the function, and returns a list of both
the data and the report. The report alone can be easily accessed in the
following way:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{interpolate}\NormalTok{(data, }
            \AttributeTok{method =} \StringTok{"approx"}\NormalTok{, }
            \AttributeTok{report =} \ConstantTok{TRUE}\NormalTok{)[[}\DecValTok{2}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  pID missing_perc_before missing_perc_after
1 118          0.02314313        0.001157156
2 119          0.01214128        0.000000000
\end{verbatim}

As shown, not all missing data has been replaced, since there are
certain periods in which the missing data span a period longer than the
default setting of the ``maxgap'' parameter, which is 150 ms.

Once interpolation has been performed, a common step is to smooth the
eye data to minimise the effect of measurement error on the data. The
function \texttt{smoother()} reduces the noise in the data by applying a
moving average function. The degree of smoothing can be specified, and a
plot can be generated (using data from a randomly selected trial) to
observe how well the smoothed data fits the raw data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{smoother}\NormalTok{(data,}
                 \AttributeTok{span =}\NormalTok{ .}\DecValTok{02}\NormalTok{,}
                 \AttributeTok{plot =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Working with eyetools}\label{working-with-eyetools}

Having explained these rudimentary steps of getting the data ready for
the main analysis, we will now describe the core functions available in
the latest version of eyetools. For illustration, eyetools has a built
in data set that meets the required format. The data set consists of
data from two participants from a few trials of a human causal learning
study (\citeproc{ref-beesley2015}{Beesley et al., 2015}). The nature of
this experiment is largely unimportant for the current purposes, but for
clarity, the data were collected from the decision period of the
procedure, where two rectangular cue stimuli were presented in the top
half of the screen, one on the left side and one on the right side. Two
smaller response options were presented in the lower half of the screen,
one above the other. Participants simply had to look at the cues and
choose a response. These raw data can be accessed by calling
\texttt{HCL} and the associated ``areas of interest'' (described later)
can be called with \texttt{HCL\_AOIs}.

\subsubsection{Counterbalanced designs}\label{counterbalanced-designs}

Many psychology experiments will counterbalance the position of
important stimuli on the screen. For example, in the example data we are
using, there are two stimuli, with one of these appearing on the left
side of the screen and the other on the right. In the design of the
experiment, one of these stimuli can be considered a ``target'' and the
other a ``distractor'', and the experiment counterbalances whether these
are positioned in a left/right or a right/left arrangement across
trials. In order to provide a meaningful analysis of the eye position
over all trials, it is necessary to standardise the data, such that the
resulting analyses reflect meaningful eye gaze on each type of stimulus
(target or distractor).

\emph{eyetools} has a built in function,
\texttt{conditional\_transform()}, which allows us to \emph{transform}
the x and/or y values of the stimuli so as to take into account a
counterbalancing variable. This function currently allows for a
single-dimensional transformation, across either the horizontal or
vertical midline. It can be used on raw data or fixation data; we simply
need to append a column to the data to reflect the counterbalancing
variable. The result of the function is a set of data in which the x
(and/or y) position is consistent across counterbalanced conditions
(e.g., in our example, we can transform the data so that the target cue
is always on the left). This transformation is especially useful for
future visualisations and calculation of time on areas of interest.

In our example data, the stimuli were presented on either the left or
the right side of the screen. Here we have merged the eye data with a
set of ``trial\_events'' data that describe the events on each trial. We
can apply \texttt{conditional\_transform()} and specify the relevant
column (cue\_order) that controls the counterbalancing, and the relevant
value that signals a switch of position (here the value ``2''). The
resulting transformation of the data will mean that the data is
normalised such that the target stimulus is always positioned on the
left side of the screen.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# merges with the common variables pNum and trial}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{merge}\NormalTok{(data, HCL\_behavioural) }

\CommentTok{\# perform a transformation of the data across the x coordinate midline}
\CommentTok{\# for all trials with value 2 in the column cue\_order}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{conditional\_transform}\NormalTok{(data, }
                              \AttributeTok{flip =} \StringTok{"x"}\NormalTok{, }
                              \AttributeTok{cond\_column =} \StringTok{"cue\_order"}\NormalTok{, }
                              \AttributeTok{cond\_values =} \StringTok{"2"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\subsection{Fixations}\label{fixations}

Once the data has been repaired and smoothed, a core step in eye data
analysis is to identify fixations (\citeproc{ref-salvucci2000}{Salvucci
\& Goldberg, 2000}). Broadly, a fixation is defined as a period in which
gaze stops in a specific location for a given amount of time. The period
in which the eyes are moving between fixations reflects a ``saccade''.
As such, raw data can be transformed into these meaningful eye data
characteristics. These different properties of eye-data have important
implications for behavioural research (see X for a review). Beyond their
importance for understanding psychological processes, transforming the
data into fixations and saccades leads to greater computational
efficiency. For example, the built in HCL data in eyetools is 479 kb,
which contains 31,041 rows of data (12 trials of data). After processing
the data for fixations, the resulting data is 269 rows and can be saved
as 3.8 kb (less than 1\% the size of the raw data). Not only is this
more computationally efficient, but it also means the data are now in a
far more practical format for storage in online data repositories.

There are two fixation algorithms offered in the \emph{eyetools}
package, both based on methods presented by
(\citeproc{ref-salvucci2000}{Salvucci \& Goldberg, 2000}). The first,
\texttt{fixation\_dispersion()} seeks periods of low variability in the
spatial component of the data; the algorithm looks for sufficient
periods of time in which the gaze position remains within a tolerated
maximum range of dispersion. Once this range is exceeded, this is deemed
to be the end of a possible period of fixation. If the total time of
this fixation period is longer than the minimum required (set by the
\texttt{min\_dur} parameter), then the fixation is stored as a record in
the returned object.

The second algorithm, \texttt{fixation\_VTI()} , employs a
velocity-threshold approach to identifying fixations, based on the
algorithm described in (\citeproc{ref-salvucci2000}{Salvucci \&
Goldberg, 2000}). Since points of fixation occur when the eye is not in
consistent motion, the algorithm computes the euclidean distance between
points and then determines the velocity of the eye. Periods in which
this velocity is consistently below the velocity threshold (for which
the default is 100 degrees of visual angle per second) are identified as
a potential period of fixation. The algorithm then applies a dispersion
check to ensure that the eye maintains a relatively stable position
across this period. Fixations must be of a minimum length for
classification (by default 150 ms).

Here we can see the example data passed to the
\texttt{fixation\_dispersion()} algorithm and the resulting fixations
that are returned.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fixations }\OtherTok{\textless{}{-}} 
  \FunctionTok{fixation\_dispersion}\NormalTok{(data,}
                      \AttributeTok{min\_dur =} \DecValTok{150}\NormalTok{, }\CommentTok{\# Min duration in ms}
                      \AttributeTok{disp\_tol =} \DecValTok{100}\NormalTok{, }\CommentTok{\# Max dispersion tolerance in pixels}
                      \AttributeTok{NA\_tol =} \FloatTok{0.25}\NormalTok{, }\CommentTok{\# proportion of NAs tolerated }
                      \AttributeTok{progress =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# toggle progress bar}
                   

\FunctionTok{head}\NormalTok{(fixations, }\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  pID trial fix_n start  end duration    x   y prop_NA min_dur disp_tol
1 118     1     1     0  173      173  959 811       0     150      100
2 118     1     2   197  397      200  961 590       0     150      100
3 118     1     3   400  653      253  958 490       0     150      100
4 118     1     4   803 1083      280 1372 839       0     150      100
\end{verbatim}

\subsection{Saccades}\label{saccades}

Between periods of fixation, the velocity of the eye increases rapidly
as it makes a saccade towards the next point of fixation. The
\texttt{saccade\_VTI()} function will extract saccades using the
velocity threshold algorithm described above. Details of each saccade
are given, such as the timing of the saccade onset, duration, and the
origin and terminal coordinates.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{saccades }\OtherTok{\textless{}{-}} \FunctionTok{saccade\_VTI}\NormalTok{(data,}
                        \AttributeTok{threshold =} \DecValTok{150}\NormalTok{,}
                        \AttributeTok{min\_dur =} \DecValTok{20}\NormalTok{)}

\FunctionTok{head}\NormalTok{(saccades, }\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  pID trial sac_n start  end duration origin_x origin_y terminal_x terminal_y
1 118     1     1  2180 2240       60 833.2688 296.7871   487.3967   705.9158
2 118     1     2  2710 2750       40 614.5028 605.7001   862.3837   408.3421
3 118     1     3  3673 3726       53 885.6256 253.4150   558.1883   655.7776
4 118     1     4  4213 4233       20 460.3286 722.8386   577.2034   617.8567
  mean_velocity peak_velocity
1      225.0736      331.8455
2      200.3353      263.8863
3      243.7927      340.3059
4      195.6512      251.7763
\end{verbatim}

\subsection{Area of interest (AOI)
analysis}\label{area-of-interest-aoi-analysis}

A critical component of much eye gaze analysis is the assessment of time
spent in regions of space. \emph{eyetools} has a number of functions for
assessing the time spent in Areas of Interest (AOIs), as well as the
sequence in which the eye enters and exits these areas. AOIs will
typically reflect regions of space in which critical stimuli appear.
AOIs are defined in eyetools using a dataframe object, which codes the
centrepoint of the AOI in x, y coordinates along with the width and
height (if the AOIs are rectangular) or just the radius (if circular).
This object can be created using the function
\texttt{create\_AOI\_df()}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set areas of interest}
\NormalTok{AOI\_areas }\OtherTok{\textless{}{-}} \FunctionTok{create\_AOI\_df}\NormalTok{(}\DecValTok{3}\NormalTok{)}

\NormalTok{AOI\_areas[}\DecValTok{1}\NormalTok{,] }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{460}\NormalTok{, }\DecValTok{840}\NormalTok{, }\DecValTok{400}\NormalTok{, }\DecValTok{300}\NormalTok{) }\CommentTok{\# Left rectangualar AOI}
\NormalTok{AOI\_areas[}\DecValTok{2}\NormalTok{,] }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1460}\NormalTok{, }\DecValTok{840}\NormalTok{, }\DecValTok{200}\NormalTok{, }\ConstantTok{NA}\NormalTok{) }\CommentTok{\# Right circular AOI}
\NormalTok{AOI\_areas[}\DecValTok{3}\NormalTok{,] }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{960}\NormalTok{, }\DecValTok{840}\NormalTok{, }\DecValTok{200}\NormalTok{, }\DecValTok{400}\NormalTok{) }\CommentTok{\# Right circular AOI}
\end{Highlighting}
\end{Shaded}

Time spent in AOIs can also be calculated from fixations or raw data
using the \texttt{AOI\_time()} function. This calculates the time spent
in each AOI per trial. Output can be expressed in the form of absolute
time, or by passing a vector of times to the ``trial\_time'' parameter,
can be expressed as proportional time.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_AOI\_time }\OtherTok{\textless{}{-}} 
  \FunctionTok{AOI\_time}\NormalTok{(}\AttributeTok{data =}\NormalTok{ fixations, }
           \AttributeTok{data\_type =} \StringTok{"fix"}\NormalTok{, }
           \AttributeTok{AOIs =}\NormalTok{ HCL\_AOIs, }
           \AttributeTok{AOI\_names =} \FunctionTok{c}\NormalTok{(}\StringTok{"target"}\NormalTok{, }\StringTok{"distractor"}\NormalTok{, }\StringTok{"outcomes"}\NormalTok{),}
           \AttributeTok{as\_prop =} \ConstantTok{TRUE}\NormalTok{,}
           \AttributeTok{trial\_time =}\NormalTok{ HCL\_behavioural}\SpecialCharTok{$}\NormalTok{RT) }

\FunctionTok{head}\NormalTok{(data\_AOI\_time, }\DecValTok{9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The \texttt{AOI\_time\_binned()} function can assess total duration of
time on AOIs, divided into sequential time bins. Since fixations will
naturally overlap these segments in many circumstances, this functions
works only with raw data. Here we are assessing time in the three AOIs
for periods of 1000 ms, limiting analysis to the first 8000 ms.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_AOI\_time\_binned }\OtherTok{\textless{}{-}} 
  \FunctionTok{AOI\_time\_binned}\NormalTok{(data, }
                  \AttributeTok{AOIs =}\NormalTok{ HCL\_AOIs,}
                  \AttributeTok{AOI\_names =} \FunctionTok{c}\NormalTok{(}\StringTok{"predictive"}\NormalTok{, }\StringTok{"non{-}predictive"}\NormalTok{, }\StringTok{"target"}\NormalTok{),}
                  \AttributeTok{bin\_length =} \DecValTok{1000}\NormalTok{,}
                  \AttributeTok{max\_time =} \DecValTok{8000}\NormalTok{) }\CommentTok{\#in milliseconds}
\end{Highlighting}
\end{Shaded}

It is also possible to determine the sequence of entries into AOIs using
the \texttt{AOI\_seq()} function. This function currently works only
with fixation data. For a given trial, the sequence of fixations is
assessed against the AOIs provided, where consecutive fixations within
the same AOI are combined into one ``entry period''. The result of this
function is a sequence of AOI entries per trial for each participant,
providing data on the sampling order of AOIs. The resulting output
provides start and end times and duration of each entry.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_AOI\_entry }\OtherTok{\textless{}{-}} 
  \FunctionTok{AOI\_seq}\NormalTok{(fixations, }
          \AttributeTok{AOIs =}\NormalTok{ HCL\_AOIs,}
          \AttributeTok{AOI\_names =} \FunctionTok{c}\NormalTok{(}\StringTok{"target"}\NormalTok{, }\StringTok{"distractor"}\NormalTok{, }\StringTok{"outcomes"}\NormalTok{))}

\FunctionTok{head}\NormalTok{(data\_AOI\_entry, }\DecValTok{9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Visualisations}\label{visualisations}

The eyetools package has a number of built in visualisations that allow
for functional plots of the data, with minimal effort. All plots use the
dominant graphical R package \texttt{ggplot}, which means that the
resulting plots from these functions are ggplot objects and can
therefore be customised using the full suite of options for ggplot and
its extensions.

\texttt{plot\_spatial()} offers a simple means to view the data produced
by \emph{eyetools}. By default this will plot all of the data that is
passed to the function, but participant IDs and trial values can be
specified in order to plot specific data. Here we plot the raw data from
a single trial for one participant, with the detected fixations
overlaid. When using fixation data, the fixations are labelled in their
temporal order (by default), enabling a clear presentation of how the
fixations arose.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# will tidy this up once eyetools functions can filter to individual participants}

\FunctionTok{plot\_spatial}\NormalTok{(}\AttributeTok{raw\_data =}\NormalTok{ data,}
             \AttributeTok{fix\_data =}\NormalTok{ fixations,}
             \AttributeTok{pID\_values =} \DecValTok{118}\NormalTok{,}
             \AttributeTok{trial\_values =} \DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{BRM_ms_files/figure-pdf/unnamed-chunk-13-1.pdf}}

In addition to eye data, an image can be added to the plot, which is
useful for inspecting data over a representation of the experimental
task. If AOIs have been defined, these can be plotted as well. Here we
demonstrate the plotting of the saccades, AOIs, and a background image:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_spatial}\NormalTok{(}\AttributeTok{sac\_data =}\NormalTok{ saccades,}
             \AttributeTok{AOIs =}\NormalTok{ HCL\_AOIs,}
             \AttributeTok{pID\_values =} \DecValTok{118}\NormalTok{,}
             \AttributeTok{trial\_values =} \DecValTok{6}\NormalTok{,}
             \AttributeTok{bg\_image =} \StringTok{"images/HCL\_sample\_image.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{BRM_ms_files/figure-pdf/unnamed-chunk-14-1.pdf}}

The function \texttt{plot\_seq()} is useful for visualising example data
as a series of plots, mapping out eye movements over the course of a
single trial. By default this function will plot a randomly selected
trial from the raw data that is passed to the function. Otherwise,
specific trials and participant values can be specified. The function
requires a ``bin\_time'' parameter, that specifies the length of each
time-period within the trial. An optional parameter of ``bin\_range''
can be specified to restrict the range of these periods that are
presented. For example here we plot data in periods of 1000 ms across
the first four of these periods.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_seq}\NormalTok{(}\AttributeTok{data =}\NormalTok{ data,}
         \AttributeTok{bin\_time =} \DecValTok{1000}\NormalTok{,}
         \AttributeTok{bin\_range =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{),}
         \AttributeTok{trial\_values =} \DecValTok{1}\NormalTok{,}
         \AttributeTok{pID\_values =} \DecValTok{118}\NormalTok{,}
         \AttributeTok{AOIs =}\NormalTok{ HCL\_AOIs,}
         \AttributeTok{bg\_image =} \StringTok{"images/HCL\_sample\_image.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The \texttt{plot\_AOI\_growth()} function offers the representation of
how an individual (on a single trial) spends their time looking at the
different AOIs. This can be useful to see how AOIs are interacted with
over time, and this can be presented as either a cumulative over time,
or as a proportion of the time spent in the trial.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot absolute and then proportional}
\FunctionTok{plot\_AOI\_growth}\NormalTok{(}\AttributeTok{data =}\NormalTok{ data, }
                \AttributeTok{AOIs =}\NormalTok{ HCL\_AOIs, }
                \AttributeTok{type =} \StringTok{"abs"}\NormalTok{, }
                \AttributeTok{pID\_values =} \DecValTok{118}\NormalTok{,}
                \AttributeTok{trial\_values =} \DecValTok{1}\NormalTok{)}
\FunctionTok{plot\_AOI\_growth}\NormalTok{(}\AttributeTok{data =}\NormalTok{ data, }
                \AttributeTok{AOIs =}\NormalTok{ HCL\_AOIs, }
                \AttributeTok{type =} \StringTok{"prop"}\NormalTok{,  }
                \AttributeTok{pID\_values =} \DecValTok{118}\NormalTok{,}
                \AttributeTok{trial\_values =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\caption{\label{fig-growth}Examples of the absolute and proportional
time plots from \texttt{plot\_AOI\_growth()}}

\end{figure}%

A heatmap of eye gaze positions can be generated using
\texttt{plot\_heatmap()} which takes raw data as an input. As a
function, and unlike many of the processing steps, it does not
differentiate between trials or participants and plots any coordinate
data it is given. This behaviour is allowed as the heatmap offers an
excellent and fast ``sanity check'' that participants were, on the
whole, looking at the expected areas of the experiment screen during the
trials. As can be seen in Figure \textbf{?@fig-heatmap}, we can be
reassured that participants do indeed spend most of their time looking
at the stimuli on screen rather than in the empty space.
\texttt{plot\_heatmap()} also allows for the modification of the amount
of data displayed, using the \texttt{alpha\_control} parameter. By
decreasing \texttt{alpha\_control} in Figure
\textbf{?@fig-heatmap-alpha-update}, we gain more visualised information
and we can still see that the majority of the data is kept within the
stimuli and saccades between these areas.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_heatmap}\NormalTok{(data, }\AttributeTok{bg\_image =} \StringTok{"images/HCL\_sample\_image.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{eyetool assumptions {[}I DON'T KNOW WHERE THIS SHOULD GO
JUST
YET{]}}\label{eyetool-assumptions-i-dont-know-where-this-should-go-just-yet}

As with any data processing or analysis, there are certain assumptions
made when developing the eyetools package. Some of these are built into
the package directly, either as errors or warnings, such as the
assumption that data is ordered by participant ID (if present) and
trial, and some are not built in because they would limit the
flexibility of the package functionality. One built-in assumption is the
handling of missing data. eyetools expects track loss to be represented
as NA within the data, and so any system that provides a different
convention for recording track loss needs to be changed prior to using
eyetools functions.

During development, eyetools was tested using data collected from a
Tobii Pro Spectrum eye tracker recording at 300Hz. Screen resolutions
were constant at 1080x1920 pixels, and the timestamps were recorded in
milliseconds. Whilst most of the functions were designed to work with
any hardware provided the data is formatted to eyetools expectations
(with the exception of \texttt{hdf5\_to\_df()} and
\texttt{hdf5\_get\_event()} as these convert Tobii data), as well as not
relying on specific frequencies or resolutions (either through the
function behaviour, or by supplying parameters for specificity),
eyetools has not been tested on a diverse set of datasets.

Some default behaviours are in-built, but are easily overrided such as
parameters for resolution in the plotting functions. Similarly
\texttt{saccade\_VTI()} and \texttt{fixation\_VTI()} were tested with
300Hz data. For these functions, as the frequency increases, the
relative saccadic velocities will be lower meaning that the thresholds
need to be reduced. This is important to note when working with data
that is not recorded at 300Hz. To circumvent the potential issue of
sample rates being an issue, by default functions that require a sample
rate will deduce the frequency from the data rather than needing it to
be specified.

\section{Analysing eye data}\label{analysing-eye-data}

@tom

\section{Discussion}\label{discussion}

In the present tutorial, we began by identifying the current gap in
available tools for working with eye data in open-science pipelines. We
then provided an overview of the general data collection process
required for eye tracking research, before detailing the conversion of
raw eye data into a useable \emph{eyetools} format. We then covered the
entire processing pipeline using functions available in the
\emph{eyetools} package that included the repairing and normalising the
data, and the detection of events such as fixations, saccades, and AOI
entries. @SOMETHING\_ON\_THE\_ANALYSIS\_GOES\_HERE.

From a practical perspective, this tutorial offers a step-by-step
walkthrough for handling eye data using R for open-science, reproducible
purposes. It provides a pipeline that can be relied upon by novices
looking to work with eye data, as well as offering new functions and
tools for experienced researchers. By enabling the processing and
analysis of data in a single R environment it also helps to speed up
data analysis.

\subsection{Advantages of Open-Source
Tools}\label{advantages-of-open-source-tools}

eyetools offers an open-source toolset that holds no hidden nor
proprietary functionality. The major benefits of open-source tools are
extensive, but the main ones include the ability to explore and engage
with the underlying functions to ensure that

A collaborative community - with open source tools, if an unmet need is
identified, then the community can work to provide a solution.

\subsection{Good Science Practices with
eyetools}\label{good-science-practices-with-eyetools}

Creating savepoints (like having processed raw data, and then
post-fixation calculation). Reduces the need to completely rework
workflows if an issue is detected as savepoints can be used to ensure
that computationally-intense or time-heavy processes are conducted as
infrequently as possible.

\section{Data Availability}\label{data-availability}

The data required for reproducing this tutorial is available at: @URL. A
condensed version of the dataset (starting with the
\texttt{combine\_eyes()} function) is a dataset in the \emph{eyetools}
package called HCL.

\section{Code Availability}\label{code-availability}

The code used in this tutorial is available in the reproducible
manuscript file available at:(IF STORING IN GITHUB, THEN WE NEED TO
CREATE A ZENODO SNAPSHOT FOR A DOI RATHER THAN JUST A GITHUB LINK)

\section{References}\label{references}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-beesley2015}
Beesley, T., Nguyen, K. P., Pearson, D., \& Le Pelley, M. E. (2015).
Uncertainty and predictiveness determine attention to cues during human
associative learning. \emph{Quarterly Journal of Experimental
Psychology}, \emph{68}(11), 2175--2199.
\url{https://doi.org/10.1080/17470218.2015.1009919}

\bibitem[\citeproctext]{ref-beesley2019}
Beesley, T., Pearson, D., \& Le Pelley, M. (2019). \emph{Chapter 1 - eye
tracking as a tool for examining cognitive processes} (G. Foster, Ed.;
pp. 1--30). Academic Press.
\url{https://doi.org/10.1016/B978-0-12-813092-6.00002-2}

\bibitem[\citeproctext]{ref-salvucci2000}
Salvucci, D. D., \& Goldberg, J. H. (2000). \emph{the symposium}.
71--78. \url{https://doi.org/10.1145/355017.355028}

\end{CSLReferences}






\end{document}
