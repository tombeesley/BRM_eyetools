---
title: "eyetools: an R package for simplified analysis of eye data"
shorttitle: "eyetools: eye data analysis"
author:
  - name: Tom Beesley
    corresponding: true
    orcid: 0000-0003-2836-2743
    email: t.beesley@lancaster.ac.uk
    affiliation: 
      - ref: LU
  - name: Matthew Ivory 
    affiliation: 
      - ref: LU
affiliations:
  - id: LU
    name: Lancaster University
    country: UK
    address: Department of Psychology, Lancaster University, UK, LA1 4YD
abstract: |
  | Abstract goes here 
keywords: [eye-tracking; fixations; saccades; areas-of-interest]
bibliography: bibliography.bib
format:
  apaquarto-pdf:
    link-citations: true
    numbered-lines: true
    fig-pos: 'H'
    documentmode: man
    keep-tex: true
    citeproc: false
    filters: 
      - at: pre-render
        path: "wordcount.lua"
execute:
  echo: false
  warning: false
editor_options: 
  chunk_output_type: console
floatsintext: true
 
---

```{r, include=FALSE, message=FALSE}

#install_github("tombeesley/eyetools@0.7.3.9000")
library(eyetools)

```


# Introduction

Eye tracking is a widely used technique in behavioural sciences, such as attention, learning, and decision making [@orquin2019primer]. By recording the movement of an individual's gaze during research studies, researchers can quantify where and how long individual's look at stimuli presented on screen. Eye tracking offers an insight into real-time cognitive processing, allowing researchers to infer mental states and processes with high temporal precision [@duchowski2017eye]. The most common eye tracking systems use infrared light to detect reflections from the eyes enabling the calculation of gaze position on a screen. Once this data has been collected, it can be classified into fixations and saccades. Fixations are time periods where the eyes remain relatively stationary which enable focused attention on stimuli [@rayner1998eye]. Conversely, saccades are the rapid eye movements between fixations that are typically too fast to allow for new information processing [@holmqvist2011eye].

A core element to utilising eye data effectively is in knowing how to process and analyse the information gathered, which is often comprised of hundreds of trials from multiple participants, and so datasets need to have processing stages applied consistently across participants and trials. Utilising reproducible workflows, such as implementing R-based pipelines for processing and analysis can offer this consistency as well as confidence in the outputs and results. Using R enables access to the Comprehensive R Archive Network (CRAN), a repository of packages containing functions designed to improve and optimise data processing and analysis workflows. It is here that we introduce eyetools, an R package designed to work with eye data and streamlines the processing and analysis pipeline.

The aim of this tutorial is to demonstrate the functionality contained within eyetools. It should appeal to researchers unfamiliar with working with eye data, as we detail steps of converting raw data through to the analysis in a reproducible R environment. It should also appeal to researchers accustomed to working with eye data in other environments who wish to transfer to working in R. In using eyetools as the foundation data pipeline, we hope that this tutorial provides a comprehensive and clear approach to working with eye data.

eyetools is not the only R package available for working with eye data, a recent assessment of CRAN identified seven active packages that offer eye data relevant functions. eyeTrackr offers functionality for experiments from 'Eyelink' trackers, limiting the usability of the package. Similarly, eyelinker and eyelinkReader are constrained to eyelink data. In contrast, eyetools provides a hardware-agnostic design, relying on data being in a given format that is easily managed from any data source. The emov package offers a limited set of functions, including a fixation algorithm that is based upon the same algorithm used in eyetools, however emov is limited in continued functionality, and plotting and data preparation functions are absent. eyeRead is a package exclusively designed for eye tracking on reading exercises and eyetools is designed for attention and intent-based research. saccadr offers exclusively methods for extracting saccades, limiting its usage for broader processing steps. Finally, the eyetrackingR is perhaps the most comprehensive alternative package, but it lacks any functions related to fixations and saccades, instead offering a suite of more naive approaches.

This tutorial is separated into five distinct sections. In the first section, we briefly describe the basic methodology of collecting eye data in general, and in regard to the specific dataset we use to illustrate all the functionality of the eyetools package. The second section covers the process for getting data from an eye tracker into an eyetools-friendly format. The third section introduces the foundational functions of the eyetools package, from fixing missing data, smoothing erratic gaze patterns, to calculating fixations, and detecting time spent in Areas of Interest (AOIs). The fourth section takes the processed data, and applies basic analysis techniques commonplace in eye data research. In the fifth and final section, we reflect on the benefits of the eyetools package, including contributions to open science practices, reproducibility, and providing clarity to eye data analysis.

# Data Collection

First describe basic paradigms for collecting eye data. Also purpose etc. \@tom

Then describe the specifics of the dataset we are using - I presume this is the HCL dataset in full? Should make mention of the fact that the workflow can be done either with the full dataset, or the two participant dataset provided in package.

# Converting Raw Data
\@matthew, \@tom

Owing to the vast range of eye tracking hardware available, eyetools does not offer much in the way of converting raw data into the eyetools format of participant ID, trial number, timestamp, x, and y coordinates. In this section, we cover the stages of transforming the data from a TOBII eye tracker.

TOBII-generated data is stored in an hdf5 format, which is a gold-standard method for managing large hierarchical datasets, such as eye gaze data, however due to its hierarchical, and somewhat complex structure, hdf5 is not easily worked with in R. eyetools contains two functions that convert the data into dataframes for easier data processing. `hdf5_to_df()` takes the TOBII-generated data and extracts any eye gaze information into a dataframe (or list of dataframes). When using a TOBII eyetracker in custom experimental code (such as PsychPy), it is often beneficial to store timestamps of trial beginning and end, as well as markers for specific phases of the task. These are stored within the hdf5 file and can be accessed using the `hdf5_get_event()` function. 

When you have data that is in a binocular format, that is you have a set of coordinates for each eye, it needs to be converted into single x and y coordinates for both eyes combined. Using eyetools, this can be done in one of two ways, either taking an average of the coordinates from the two eyes, or by choosing the eye with the fewest missing samples is used to represent the data. An averaging of the two coordinates sets is the typical method of combining binocular data, and can be done using the `combine_eyes()` function in eyetools. This returns a flattened list of participant data that has x and y variables in place of the left\_\* and right\_\* variables.

```{r}
data_combined <- combine_eyes(HCL,method = "average")
```


# Working with eyetools 

In the previous section, we finished with the data in a format that holds participant ID, trial number, a timestamp, along with x and y coordinates. This is the format expected by eyetools when working with multi-participant data, however if you some reason you are working with a single participant then the participant ID column is superfluous and can be dropped. This basic data format of ID, trial, time, x, and y ensures that eyetools is applicable to a variety of eye data sources and does not depend on specific eye trackers being used.

### Counterbalanced designs

Many psychology experiments will position stimuli on the screen in a counterbalanced fashion. For example, in the example data we are using, there are two stimuli, with one of these appearing on the left and one on the right. In our design, one of the cue stimuli is a "target" and one is a "distractor", and the experiment counterbalances whether these are positioned on the left or right across trials.

Eyetools has a built in function which allows us to transform the x (or y) values of the stimuli to take into account a counterbalancing variable: `conditional_transform()`. This function currently allows for a single-dimensional flip across either the horizontal or vertical midline. It can be used on raw data or fixation data. It requires the spatial coordinates (x, y) and a specification of the counterbalancing variable. The result is a normalised set of data, in which the x (and/or y) position is consistent across counterbalanced conditions (e.g., in our example, we can transform the data so that the target cue is always on the left). This transformation is especially useful for future visualisations and calculation of time on areas of interest. Note that `conditional_transform()` is another function that does not discriminate between multi-participant and single-participant data and so no participant_ID parameter is required. To transform the data, we require knowledge of where predictive cues were presented. Using this, `conditional_transform()` can align data across the x or y midline, depending on how stimuli were presented. In the experimental design used in our study, cues were presented either on the left or the right, so by applying `conditional_transform()`, all the predictive cues in the dataset are recorded on the same side.

```{r}

data_merged <- merge(data_combined, HCL_behavioural) # merges with the common variables pNum and trial

data_counterbalanced <- conditional_transform(data_merged, 
                              flip = "x", #flip across x midline
                              cond_column = "cue_order", #this column holds the counterbalance information
                              cond_values = "2",#which values in cond_column to flip
                              message = FALSE) #suppress message

```

## Repairing missing data and smoothing data

Despite researcher's best efforts and hopes, participants are likely to blink during data collection, resulting in observations where no data is present for where the eyes would be looking. To mitigate this issue, the `interpolate()` function estimates the  path taken by the eyes based upon the eye coordinates before and after the missing data. There are two methods for estimating the path, linear interpolation ("approx", the default setting) and cubic spline ("spline"). The default method of linear interpolation replaces missing values with a line of constant slope and evenly spaced coordinates reaching the existing data. The cubic spline method applies piecewise cubic functions to enable a curve to be calculated as opposed to a line between points.

```{r}
data <- interpolate(data_counterbalanced, 
                    method = "approx",
                    participant_ID = "pNum")

```

When using `interpolate()`, a report can be requested so that a researcher can measure how much missing data has been replaced. This parameter changes the output format of the function, and returns a list of both the data and the report. The report can be easily accessed using the following code:

```{r}
interpolate(data_counterbalanced, 
            method = "approx",
            participant_ID = "pNum", 
            report = TRUE)[[2]]
```

As shown, not all missing data has been replaced, this is because when gaps are larger than a given size they are kept as missing data due to it being unreasonable to try to estimate the path taken by the eye. The amount of missing data that will be estimated can be changed through the maxgap parameter. 

Once missing data has been fixed, a common step is to smooth the eye data to remove particularly jerky eye movements. To do this,  `smoother()`  reduces the noise in the data by applying a moving averaging function. The degree of smoothing can be specified, as well as having a plot generated for random trials to observe how well the smoothed data fits the raw data.

```{r, message=FALSE}
data <- smoother(data,
                 span = .02,
                 participant_ID = "pNum", 
                 plot = TRUE)
```

## Fixations

Once the data has been repaired and smoothed, a core step in eye data analysis is to identify fixations [@salvucci2000identifying], defined as when the gaze stops in a specific location for a given amount of time. When the eyes are moving between these fixations, they are considered to be saccades. Subsequently, data can be split into these two groups, fixations and saccades. In the eyetools package, there are two fixation algorithms offered; the first algorithm, `fixation_dispersion()` employs a dispersion-based approach that uses spatial and temporal data to determine fixations. By using a maximum dispersion range, the algorithm looks for sufficient periods of time that the eye gaze remains within this range and once this range is exceed, this is termed as a fixation. The second algorithm, `fixation_VTI()` takes advantage of the idea that eye data is either a fixation or a saccade and employs a velocity-threshold approach. It identifies data where the eye is moving at a minimum velocity and excludes this, before applying a dispersion check to ensure that the eye does not drift during the fixation period. If the range is broken, a new fixation is determined. Saccades must be of a given length to be removed, otherwise they are considered as micro-saccades [\@CITATION_NEEDED_HERE?].

```{r}
fixations <- fixation_dispersion(data,
                                 min_dur = 150, # Minimum duration (in milliseconds) of fixations
                                 disp_tol = 100, # Maximum tolerance (in pixels) for the dispersion of values
                                 run_interp = FALSE, # the default is true, but we have already run interpolate()
                                 NA_tol = 0.25, # the proportion of NAs tolerated within a fixation window
                                 progress = FALSE, # whether to display a progress bar or not
                                 participant_ID = "pNum") 
```

Additionally, in certain analyses it may be useful to extract the saccades themselves. This can be achieved using the `saccade_VTI()` function.

```{r}
saccades <- saccade_VTI(data, 
                        threshold = 150, 
                        min_dur = 20, 
                        participant_ID = "pNum")
```


Once fixations have been calculated, they can be used in conjunction with Areas of Interest (AOIs) to determine the sequence in which the eye enters and exits these areas, as well as the time spent in these regions. When referring to AOIs, these often refer to the cues presented and the outcome object. In our example, the two cues at the top of the screen are the cues, and the outcome is at the bottom. We can define these areas in a separate dataframe object by giving the centrepoint of the AOI in x, y coordinates along with the width and height (if the AOIs are rectangular) or just the radius (if circular).

```{r}

# set areas of interest
AOI_areas <- data.frame(matrix(nrow = 3, ncol = 4))
colnames(AOI_areas) <- c("x", "y", "width_radius", "height")

AOI_areas[1,] <- c(460, 840, 400, 300) # Left cue
AOI_areas[2,] <- c(1460, 840, 400, 300) # Right cue
AOI_areas[3,] <- c(960, 270, 300, 500) # outcomes

```

In combination with the fixation data, the AOI information can be used to determine the sequence of AOI entries using the `AOI_seq()` function. This fucntion checks whether a fixation is detected within an AOI, and if not, it is dropped from the output, and then provides a list of the sequence of AOI entries, along with start and end timestamps, and the duration.

```{r}
data_AOI_entry <- AOI_seq(fixations, AOIs = AOI_areas,
                          AOI_names = c("predictive", "non-predictive", "target"),
                          participant_ID = "pNum")
```

Time spent in AOIs can also be calculated from fixations or raw data using the `AOI_time()` function available. This calculates the time spent in each AOI in each trial, based on the data type given, in our case fixation data.

```{r}

data_AOI_time <- AOI_time(fixations, data_type = "fix", AOIs = AOI_areas,
                          AOI_names = c("predictive", "non-predictive", "target"),
                          participant_ID = "pNum")

```

If choosing to work with the raw data, there is also the option of using `AOI_time_binned()` which allows for the trials to be split into bins of a given length, and the time spent in AOIs calculated as a result.

```{r}
data_AOI_time_binned <- AOI_time_binned(data, AOIs = AOI_areas,
                                        AOI_names = c("predictive", "non-predictive", "target"),
                                        bin_length = 100,
                                        max_time = 2000, #in milliseconds
                                        participant_ID = "pNum")
```

## eyetool assumptions [I DON'T KNOW WHERE THIS SHOULD GO JUST YET]

As with any data processing or analysis, there are certain assumptions made when developing the eyetools package. Some of these are built into the package directly, either as errors or warnings, such as the assumption that data is ordered by participant ID (if present) and trial, and some are not built in because they would limit the flexibility of the package functionality. One built-in assumption is the handling of missing data. eyetools expects track loss to be represented as NA within the data, and so any system that provides a different convention for recording track loss needs to be changed prior to using eyetools functions.

During development, eyetools was tested using data collected from a Tobii Pro Spectrum eye tracker recording at 300Hz. Screen resolutions were constant at 1080x1920 pixels, and the timestamps were recorded in milliseconds. Whilst most of the functions were designed to work with any hardware provided the data is formatted to eyetools expectations (with the exception of `hdf5_to_df()` and `hdf5_get_event()` as these convert Tobii data), as well as not relying on specific frequencies or resolutions (either through the function behaviour, or by supplying parameters for specificity), eyetools has not been tested on a diverse set of datasets. 

Some default behaviours are in-built, but are easily overrided such as parameters for resolution in the plotting functions. Similarly `saccade_VTI()` and `fixation_VTI()` were tested with 300Hz data. For these functions, as the frequency increases, the relative saccadic velocities will be lower meaning that the thresholds need to be reduced. This is important to note when working with data that is not recorded at 300Hz. To circumvent the potential issue of sample rates being an issue, by default functions that require a sample rate will deduce the frequency from the data rather than needing it to be specified.

## Visualisations made easy

When working with eye data, it can be beneficial for the researcher to familiarise themselves with the dataset. Visualising the data through graphics can help to identify meaningful patterns that are obscured when relying on statisitical analyses alone [@kabacoffActionThirdEdition2022]. Graphics are also very effective at conveying information in a way that is easily grasped by a diverse audience. eyetools offers a selection of in-built plotting functions that work with data at most stages of processing. These plots are designed to aid in the researcher's processing and data analysis. 


The `plot_AOI_growth()` function offers the representation of how an individual (on a single trial) spends their time looking at the different AOIs. This can be useful to see how AOIs are interacted with over time, and this can be presented as either a cumulative over time, or as a proportion of the time spent in the trial. 

::: {#fig-growth layout-ncol=2}

```{r}
#| label: fig-abs

# plot absolute and then proportional
plot_AOI_growth(data = data, AOIs = HCL_AOIs, type = "abs", trial_number = 1)
```


```{r}
#| label: fig-prop

plot_AOI_growth(data = data, AOIs = HCL_AOIs, type = "prop", trial_number = 1)
```

Examples of the absolute and proportional time plots from `plot_AOI_growth()`

:::

A heatmap of eye gaze positions can be generated using `plot_heatmap()` which takes raw data as an input. As a function, and unlike many of the processing steps, it does not differentiate between trials or participants and plots any coordinate data it is given. This behaviour is allowed as the heatmap offers an excellent and fast "sanity check" that participants were, on the whole, looking at the expected areas of the experiment screen during the trials. As can be seen in Figure @fig-heatmap, we can be reassured that participants do indeed spend most of their time looking at the stimuli on screen rather than in the empty space. `plot_heatmap()` also allows for the modification of the amount of data displayed, using the `alpha_control` parameter. By decreasing `alpha_control` in Figure @fig-heatmap-alpha-update, we gain more visualised information and we can still see that the majority of the data is kept within the stimuli and saccades between these areas.

```{r}
#| fig-cap: A heatmap overlaid upon a sample stimuli image demonstrating where the participants looked most over all trials
#| label: fig-heatmap

plot_heatmap(data, bg_image = "images/HCL_sample_image.jpg")
```

```{r}
#| fig-cap: A heatmap overlaid upon a sample stimuli image demonstrating where the participants looked most over all trials
#| label: fig-heatmap-alpha-update

plot_heatmap(data, alpha_control = .001)
```

The `plot_seq()` function allows for the plotting of raw data to visualise the gaze pattern from a single trial and where the gaze fell on the screen across the entire trial. @fig-seq offers an example trial split into time bins of 5000ms. This plot shows the time dimension as a change in colour that overlaps older data. This plot serves as a useful check, similar to `plot_heatmap()`, as to where the eyes spent their time, but `plot_seq()` has the benefit of showing the time dimension compared to a simple heatmap.

```{r}
#| label: fig-seq
#| fig-cap: The output from plot_seq() with included AOIs and time binned into 5 second sections

# plot raw data with bins
plot_seq(data = data[data$pNum == 118,], AOIs = HCL_AOIs,  bin_time = 5000, trial_number = 1)
```

The final plotting function in eyetools is `plot_spatial()`. This can plot raw data, fixations, and saccades, either separately or in combination. `plot_spatial()` plots the location of the eye gaze of a trial, and when given raw data is very similar to the output of `plot_seq()`, when using fixation data, then an additional parameter can be used to label the fixations in their temporal order, enabling a better presentation of how fixations arise. Finally, providing saccade data allows for the length and direction of saccades to be presented. 

::: {#fig-spatial layout-ncol=3}

```{r}
#| label: fig-spatial-raw

plot_spatial(raw_data = data[data$pNum == 118,], trial_number = 1)
```


```{r}
#| label: fig-spatial-fix

# plot just fixation data together
plot_spatial(fix_data = fixations[fixations$pNum == 118,], trial_number = 1)
```


```{r}
#| label: fig-spatial-sac

#plot saccades
plot_spatial(sac_data = saccades[saccades$pNum == 118,], trial_number = 1)

```

The three types of plot that can be created using `plot_spatial()`
:::

# Analysing eye data

\@tom

# Discussion

In the present tutorial, we began by identifying the current gap in available tools for working with eye data in open-science pipelines. We then provided an overview of the general data collection process required for eye tracking research, before detailing the conversion of raw eye data into a useable eyetools format. We then covered the entire processing pipeline using functions available in the eyetools package that included the repairing and normalising the data, and the detection of events such as fixations, saccades, and AOI entries. \@SOMETHING_ON_THE_ANALYSIS_GOES_HERE.

From a practical perspective, this tutorial offers a step-by-step walkthrough for handling eye data using R for open-science, reproducible purposes. It provides a pipeline that can be relied upon by novices looking to work with eye data, as well as offering new functions and tools for experienced researchers. By enabling the processing and analysis of data in a single R environment it also helps to speed up data analysis.

## Advantages of Open-Source Tools

eyetools offers an open-source toolset that holds no hidden nor proprietary functionality. The major benefits of open-source tools are extensive, but the main ones include the ability to explore and engage with the underlying functions to ensure that 

A collaborative community - with open source tools, if an unmet need is identified, then the community can work to provide a solution.

## Good Science Practices with eyetools

Creating savepoints (like having processed raw data, and then post-fixation calculation). Reduces the need to completely rework workflows if an issue is detected as savepoints can be used to ensure that computationally-intense or time-heavy processes are conducted as infrequently as possible.

# Data Availability

The data required for reproducing this tutorial is available at: \@URL. A condensed version of the dataset (starting with the `combine_eyes()` function) is a dataset in the eyetools package called HCL.

# Code Availability

The code used in this tutorial is available in the reproducible manuscript file available at:(IF STORING IN GITHUB, THEN WE NEED TO CREATE A ZENODO SNAPSHOT FOR A DOI RATHER THAN JUST A GITHUB LINK)


# References

<!-- References will auto-populate in the refs div below -->

::: {#refs}
:::

